# ğŸ§  Training a Multilingual Translator

A practical and extensible project for fine-tuning multilingual translation models using LoRA. Covers English â†” Chinese, English â†” Nepali, and combines both for multi-task learning on a compact LLaMA-3.2-3B base.

---

## ğŸš€ Project Highlights

- âœ… Fine-tuned translation models on:
  - **English â†” Chinese** (WMT19)
  - **English â†” Nepali** ([iamTangsang/Nepali-to-English-Translation-Dataset](https://huggingface.co/datasets/iamTangsang/Nepali-to-English-Translation-Dataset))
  - **Combined multilingual model** (Chinese â†” English â†” Nepali)
- âœ… English â†” Chinese, English â†” Nepali fine-tuning performed via **LoRA (Low-Rank Adaptation)** on top of "Qwen/Qwen2.5-0.5B"
- âœ… Combined multilingual model (Chinese â†” English â†” Nepali) fine-tuning performed via **LoRA (Low-Rank Adaptation)** on top of `meta-llama/Llama-3.2-3B`
- âœ… Used Hugging Face Transformers and Trainer APIs for reproducible training on Jupyter Notebook
- âœ… Evaluation metrics: BLEU and chrF++ (details below)

---

## ğŸ“Š Evaluation Results (Multilingual Model)

| Language Pair | BLEU Score Progression        | chrF++ Score Progression      |
|---------------|-------------------------------|-------------------------------|
| **ZH â†’ EN**   | 2.86 â†’ 18.63 â†’ **19.05**      | 14.10 â†’ 45.92 â†’ **45.81**     |
| **EN â†’ ZH**   | 0.00 â†’ - â†’ **0.00**           | 1.42 â†’ 18.80 â†’ **19.21**      |
| **NE â†’ EN**   | 0.00 â†’ 19.34 â†’ **20.42**      | 8.73 â†’ 41.94 â†’ **42.01**      |
| **EN â†’ NE**   | 0.00 â†’ - â†’ **0.00**           | 0.12 â†’ 27.90 â†’ **30.38**      |

> ğŸ“ These improvements were achieved through progressive fine-tuning with LoRA using merged datasets.

---

## ğŸ“ Notebooks

You can open and run each notebook for training or inference:

- `En-Zh.ipynb`: English â†” Chinese LoRA fine-tuning
- `En-Ne.ipynb`: English â†” Nepali LoRA fine-tuning
- `Zh-En-Ne.ipynb`: Multilingual fine-tuning with merged datasets

> ğŸ§ª All training done using Jupyter Notebook & Hugging Face Trainer  
> ğŸ§  Easily extendable to new language pairs by updating the `lang_pair` field in dataset format:  
> `{"input": ..., "output": ..., "lang_pair": "en-zh"}`

---

## âœ¨ Future Plans

- ğŸ”„ Investigate the use of **English as a bridging high-resource language** to enhance zero-shot or few-shot performance on unseen language translation between low-resource pairs (e.g., Nepali â†” Chinese via English)
- ğŸ§ª Experiment with **pivot translation** and **triangular training** setups to evaluate whether they improve generalization
- ğŸ”— Explore multilingual token alignment and shared embedding space techniques for better bridging

---

## ğŸ™ Acknowledgements

- [meta-llama/Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B) for the base model
- [WMT19](https://huggingface.co/datasets/wmt/wmt19) and [iamTangsang](https://huggingface.co/datasets/iamTangsang/Nepali-to-English-Translation-Dataset) for dataset resources

---

## ğŸ“„ License

This project is open-sourced under the **MIT License**.  
Please credit the original dataset providers and model creators when reusing.
